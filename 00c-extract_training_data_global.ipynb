{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e474db9",
   "metadata": {},
   "source": [
    "# Extract training data\n",
    "\n",
    "This notebook will extract plate kinematic data from a plate model and other data from the `source_data` directory, writing the resulting dataset to a CSV file which can then be used to train the models in the following notebooks (`01*.ipynb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbeab9",
   "metadata": {},
   "source": [
    "## Notebook options\n",
    "\n",
    "These cells set some of the important variables and definitions used throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57c3d5",
   "metadata": {},
   "source": [
    "### Select plate model\n",
    "\n",
    "To use the plate model from the published paper (Alfonso et al., 2024), set `use_provided_plate_model` to `True`. Otherwise, leave `use_provided_plate_model` as `False` and set `plate_model_name` to a valid model name for the [`plate-model-manager`](https://github.com/michaelchin/plate-model-manager/blob/4f66423b53950bf42f5dac1228e61fd1e19fdf6e/models.json) package, or set `plate_model_name` to `None` and place GPlates files in a directory named `plate_model`.\n",
    "\n",
    "| `use_provided_plate_model` | `plate_model_name` | result |\n",
    "| - | - | - |\n",
    "| `True` | Any | Use Alfonso et al., 2024 model |\n",
    "| `False` | Model name string (e.g. `\"muller2022\"`) | Use specified plate model |\n",
    "| `False` | `None` | Use files in `plate_model` directory |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b1534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://repo.gplates.org/webdav/pmm/muller2022/Rotations.zip\n",
      "The local file(s) is/are still good. Will not download again at this moment.\n",
      "downloading https://repo.gplates.org/webdav/pmm/muller2022/Topologies.zip\n",
      "The local file(s) is/are still good. Will not download again at this moment.\n",
      "downloading https://repo.gplates.org/webdav/pmm/muller2022/StaticPolygons.zip\n",
      "The local file(s) is/are still good. Will not download again at this moment.\n"
     ]
    }
   ],
   "source": [
    "use_provided_plate_model = True\n",
    "plate_model_name = \"muller2022\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1896136",
   "metadata": {},
   "source": [
    "### Set other parameters\n",
    "\n",
    "Perhaps the most important options here are `n_jobs` and `end_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow reproducibility of randomised results\n",
    "random_seed = 1234\n",
    "\n",
    "# Number of processes to use\n",
    "n_jobs = 4\n",
    "\n",
    "# Overwrite any existing output files\n",
    "overwrite = False\n",
    "\n",
    "# Timespan for analysis\n",
    "start_time = 0\n",
    "end_time = 170\n",
    "\n",
    "# Control verbosity level of logging output\n",
    "verbose = False\n",
    "\n",
    "# Number of unlabelled points to generate\n",
    "num_unlabelled = 200  # per timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee294e4e",
   "metadata": {},
   "source": [
    "If any of the following exist as environment variables, they will replace the values defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67868847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Override above values with environment variables, if they exist\n",
    "n_jobs = int(os.environ.get(\"N_JOBS\", n_jobs))\n",
    "overwrite = bool(int(os.environ.get(\"OVERWRITE\", overwrite)))\n",
    "start_time = int(os.environ.get(\"START_TIME\", start_time))\n",
    "end_time = int(os.environ.get(\"END_TIME\", end_time))\n",
    "verbose = bool(int(os.environ.get(\"VERBOSE\", verbose)))\n",
    "num_unlabelled = int(os.environ.get(\"NUM_UNLABELLED\", num_unlabelled))\n",
    "\n",
    "times = range(start_time, end_time + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa8fc32",
   "metadata": {},
   "source": [
    "## Notebook setup\n",
    "\n",
    "Imports, definitions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce1302b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92a09be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:20:03.077124Z",
     "iopub.status.busy": "2024-02-01T04:20:03.076814Z",
     "iopub.status.idle": "2024-02-01T04:20:07.283816Z",
     "shell.execute_reply": "2024-02-01T04:20:07.283250Z"
    },
    "papermill": {
     "duration": 4.216123,
     "end_time": "2024-02-01T04:20:07.285432",
     "exception": false,
     "start_time": "2024-02-01T04:20:03.069309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONWARNINGS=ignore::UserWarning\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from gplately.tools import plate_isotherm_depth\n",
    "\n",
    "from lib.assign_regions import assign_regions\n",
    "from lib.calculate_convergence import run_calculate_convergence\n",
    "from lib.check_files import (\n",
    "    check_source_data,\n",
    "    check_plate_model,\n",
    ")\n",
    "from lib.combine_point_data import combine_point_data\n",
    "from lib.coregister_combined_point_data import run_coregister_combined_point_data\n",
    "from lib.coregister_crustal_thickness import run_coregister_crustal_thickness\n",
    "from lib.coregister_ocean_rasters import run_coregister_ocean_rasters\n",
    "from lib.create_study_area_polygons import run_create_study_area_polygons\n",
    "from lib.erodep import calculate_erodep\n",
    "from lib.generate_unlabelled_points import generate_unlabelled_points\n",
    "from lib.misc import calculate_slab_flux\n",
    "from lib.plate_models import get_plate_reconstruction\n",
    "from lib.slab_dip import calculate_slab_dip\n",
    "from lib.water import calculate_water_thickness\n",
    "\n",
    "# Suppress occasional joblib warnings\n",
    "%env PYTHONWARNINGS=ignore::UserWarning\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "880580d1",
   "metadata": {
    "papermill": {
     "duration": 0.003216,
     "end_time": "2024-02-01T04:20:07.304439",
     "exception": false,
     "start_time": "2024-02-01T04:20:07.301223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Input and output files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e17a79",
   "metadata": {},
   "source": [
    "If necessary, the plate model will be downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60656f7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'use_provided_plate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plate_model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplate_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43muse_provided_plate_model\u001b[49m:\n\u001b[1;32m      3\u001b[0m     check_plate_model(plate_model_dir, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m plate_model \u001b[38;5;241m=\u001b[39m get_plate_reconstruction(\n\u001b[1;32m      5\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mplate_model_name,\n\u001b[1;32m      6\u001b[0m     model_dir\u001b[38;5;241m=\u001b[39mplate_model_dir,\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'use_provided_plate_model' is not defined"
     ]
    }
   ],
   "source": [
    "plate_model_dir = \"plate_model\"\n",
    "if use_provided_plate_model:\n",
    "    check_plate_model(plate_model_dir, verbose=True)\n",
    "    plate_model_name = None\n",
    "plate_model = get_plate_reconstruction(\n",
    "    model_name=plate_model_name,\n",
    "    model_dir=plate_model_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6b313",
   "metadata": {},
   "source": [
    "The directory containing the datasets to be extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb00231",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"source_data\"\n",
    "data_dir = check_source_data(data_dir, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbcd39d",
   "metadata": {},
   "source": [
    "Output files will be created in this directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cdb93cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"extracted_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489edaa8",
   "metadata": {},
   "source": [
    "The following input directories are all relative to `data_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d7de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file with known deposits; columns:\n",
    "# lon, lat, age (Ma)\n",
    "deposits_filename = \"deposit_data_global.csv\"\n",
    "\n",
    "# If desired, categorise deposits according to location\n",
    "# Should be a shapefile or GeoJSON containing polygons\n",
    "# with a 'region' attribute\n",
    "regions_filename = \"regions.geojson\"\n",
    "\n",
    "# Seafloor age grid directory\n",
    "# Filename format 'seafloor_age_{time}Ma.nc'\n",
    "agegrid_dir = \"AgeGrids\"\n",
    "\n",
    "# Seafloor sediment thickness directory\n",
    "# Filename format 'sediment_thickness_{time}Ma.nc'\n",
    "sedthick_dir = \"SedimentThickness\"\n",
    "\n",
    "# Seafloor carbonate sediment thickness directory\n",
    "# Filename format 'carbonate_thickness_{time}Ma.nc'\n",
    "carbonate_dir = \"CarbonateThickness\"\n",
    "\n",
    "# Oceanic crustal CO2 density directory\n",
    "# Filename format 'crustal_co2_{time}Ma.nc'\n",
    "co2_dir = \"CrustalCO2\"\n",
    "\n",
    "# Overriding plate thickness directory\n",
    "# Filename format 'crustal_thickness_{time}Ma.nc'\n",
    "crustal_thickness_dir = \"CrustalThickness\"\n",
    "\n",
    "# Cumulative subducted sediments/carbonates/etc. directory\n",
    "# Filename format 'sediment_thickness/cumulative_density_{time}Ma.nc',\n",
    "# 'carbonate_thickness/cumulative_density_{time}Ma.nc', etc.\n",
    "subducted_quantities_dir = \"SubductedQuantities\"\n",
    "\n",
    "# Erosion/deposition rate directory\n",
    "# Filename format 'erosion_deposition_{time}Ma.nc'\n",
    "erodep_dir = \"ErosionDeposition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32544941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:20:07.312123Z",
     "iopub.status.busy": "2024-02-01T04:20:07.311857Z",
     "iopub.status.idle": "2024-02-01T04:20:07.321235Z",
     "shell.execute_reply": "2024-02-01T04:20:07.320546Z"
    },
    "papermill": {
     "duration": 0.01526,
     "end_time": "2024-02-01T04:20:07.323015",
     "exception": false,
     "start_time": "2024-02-01T04:20:07.307755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Handle relative file/directory paths\n",
    "\n",
    "deposits_filename = os.path.join(data_dir, deposits_filename)\n",
    "regions_filename = os.path.join(data_dir, regions_filename)\n",
    "agegrid_dir = os.path.join(data_dir, agegrid_dir)\n",
    "sedthick_dir = os.path.join(data_dir, sedthick_dir)\n",
    "carbonate_dir = os.path.join(data_dir, carbonate_dir)\n",
    "co2_dir = os.path.join(data_dir, co2_dir)\n",
    "crustal_thickness_dir = os.path.join(data_dir, crustal_thickness_dir)\n",
    "subducted_quantities_dir = os.path.join(data_dir, subducted_quantities_dir)\n",
    "erodep_dir = os.path.join(data_dir, erodep_dir)\n",
    "\n",
    "subduction_data_filename = os.path.join(output_dir, \"subducting_plate_data.csv\")\n",
    "study_area_dir = os.path.join(output_dir, \"study_area_polygons\")\n",
    "output_filename = os.path.join(output_dir, \"training_data_global.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad4521b6",
   "metadata": {
    "papermill": {
     "duration": 0.003061,
     "end_time": "2024-02-01T04:20:07.329363",
     "exception": false,
     "start_time": "2024-02-01T04:20:07.326302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Subducting plate data\n",
    "\n",
    "This cell will extract the subduction kinematics data from the plate model, along with datasets relating to the subducting oceanic plate: seafloor age, sediment and carbonate thickness, etc.\n",
    "However, if this data has already been extracted by another notebook and `overwrite` has not been set to `True`, then the data will be read from that file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61eb7d37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:20:07.336836Z",
     "iopub.status.busy": "2024-02-01T04:20:07.336512Z",
     "iopub.status.idle": "2024-02-01T04:20:13.995286Z",
     "shell.execute_reply": "2024-02-01T04:20:13.994693Z"
    },
    "papermill": {
     "duration": 6.664926,
     "end_time": "2024-02-01T04:20:13.997275",
     "exception": false,
     "start_time": "2024-02-01T04:20:07.332349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    subduction_data_filename is not None and os.path.isfile(subduction_data_filename)\n",
    ") and (not overwrite):\n",
    "    subduction_data = pd.read_csv(subduction_data_filename)\n",
    "else:\n",
    "    subduction_data = run_calculate_convergence(\n",
    "        nprocs=n_jobs,\n",
    "        min_time=min(times),\n",
    "        max_time=max(times),\n",
    "        plate_reconstruction=plate_model,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    subduction_data = run_coregister_ocean_rasters(\n",
    "        nprocs=n_jobs,\n",
    "        times=times,\n",
    "        input_data=subduction_data,\n",
    "        agegrid_dir=agegrid_dir,\n",
    "        plate_reconstruction=plate_model,\n",
    "        sedthick_dir=sedthick_dir,\n",
    "        carbonate_dir=carbonate_dir,\n",
    "        co2_dir=co2_dir,\n",
    "        subducted_thickness_dir=os.path.join(\n",
    "            subducted_quantities_dir,\n",
    "            \"plate_thickness\",\n",
    "        ),\n",
    "        subducted_sediments_dir=os.path.join(\n",
    "            subducted_quantities_dir,\n",
    "            \"sediment_thickness\",\n",
    "        ),\n",
    "        subducted_carbonates_dir=os.path.join(\n",
    "            subducted_quantities_dir,\n",
    "            \"carbonate_thickness\",\n",
    "        ),\n",
    "        subducted_water_dir=os.path.join(\n",
    "            subducted_quantities_dir,\n",
    "            \"water_thickness\",\n",
    "        ),\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    subduction_data[\"plate_thickness (m)\"] = plate_isotherm_depth(\n",
    "        subduction_data[\"seafloor_age (Ma)\"],\n",
    "        maxiter=100,\n",
    "    )\n",
    "    subduction_data = calculate_water_thickness(data=subduction_data)\n",
    "    subduction_data = calculate_slab_flux(subduction_data)\n",
    "    subduction_data = calculate_slab_dip(subduction_data)\n",
    "\n",
    "    if subduction_data_filename is not None:\n",
    "        subduction_data.to_csv(subduction_data_filename, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c13f1fcf",
   "metadata": {
    "papermill": {
     "duration": 0.00351,
     "end_time": "2024-02-01T04:20:14.004733",
     "exception": false,
     "start_time": "2024-02-01T04:20:14.001223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create study area polygons along subduction zones\n",
    "\n",
    "Here we define our study area as all points on the overriding plate within a certain distance of the subduction zone (by default, $6 \\degree, \\approx 660\\mathrm{km}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9007fbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:20:14.012533Z",
     "iopub.status.busy": "2024-02-01T04:20:14.012285Z",
     "iopub.status.idle": "2024-02-01T04:21:17.761117Z",
     "shell.execute_reply": "2024-02-01T04:21:17.760379Z"
    },
    "papermill": {
     "duration": 63.754633,
     "end_time": "2024-02-01T04:21:17.762867",
     "exception": false,
     "start_time": "2024-02-01T04:20:14.008234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lib.create_study_area_polygons import DEFAULT_SZ_BUFFER_DISTANCE\n",
    "\n",
    "buffer_distance = DEFAULT_SZ_BUFFER_DISTANCE  # 6.0\n",
    "\n",
    "if overwrite or not os.path.isdir(study_area_dir):\n",
    "    run_create_study_area_polygons(\n",
    "        nprocs=n_jobs,\n",
    "        times=times,\n",
    "        plate_reconstruction=plate_model,\n",
    "        output_dir=study_area_dir,\n",
    "        buffer_distance=buffer_distance,\n",
    "        verbose=verbose,\n",
    "        return_output=False,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd820aa6",
   "metadata": {
    "papermill": {
     "duration": 0.003665,
     "end_time": "2024-02-01T04:21:17.770448",
     "exception": false,
     "start_time": "2024-02-01T04:21:17.766783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generate random unlabelled data points\n",
    "\n",
    "The unlabelled set is created by generating uniformly-distributed random points within the polygons created in the previous cell. To change the number of points generated at each timestep, modify the `num_unlabelled` parameter defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "469bcdb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:21:17.779562Z",
     "iopub.status.busy": "2024-02-01T04:21:17.778978Z",
     "iopub.status.idle": "2024-02-01T04:36:34.277930Z",
     "shell.execute_reply": "2024-02-01T04:36:34.277264Z"
    },
    "papermill": {
     "duration": 916.505708,
     "end_time": "2024-02-01T04:36:34.279748",
     "exception": false,
     "start_time": "2024-02-01T04:21:17.774040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unlabelled = generate_unlabelled_points(\n",
    "    times=times,\n",
    "    input_dir=study_area_dir,\n",
    "    num=num_unlabelled,\n",
    "    threads=n_jobs,\n",
    "    seed=random_seed,\n",
    "    plate_reconstruction=plate_model,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8775853",
   "metadata": {
    "papermill": {
     "duration": 0.00385,
     "end_time": "2024-02-01T04:36:34.291518",
     "exception": false,
     "start_time": "2024-02-01T04:36:34.287668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Combine labelled deposit/non-deposit data with random unlabelled data\n",
    "\n",
    "The function below wrangles the points generated in the previous cell into the same format as the deposit location data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f002d601",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:36:34.304011Z",
     "iopub.status.busy": "2024-02-01T04:36:34.303798Z",
     "iopub.status.idle": "2024-02-01T04:38:40.810774Z",
     "shell.execute_reply": "2024-02-01T04:38:40.810050Z"
    },
    "papermill": {
     "duration": 126.518327,
     "end_time": "2024-02-01T04:38:40.812572",
     "exception": false,
     "start_time": "2024-02-01T04:36:34.294245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_points = combine_point_data(\n",
    "    deposit_data=deposits_filename,\n",
    "    unlabelled_data=unlabelled,\n",
    "    plate_reconstruction=plate_model,\n",
    "    study_area_dir=study_area_dir,\n",
    "    min_time=min(times),\n",
    "    max_time=max(times),\n",
    "    n_jobs=n_jobs,\n",
    "    verbose=verbose,\n",
    ")\n",
    "del unlabelled\n",
    "combined_points = combined_points.dropna(subset=[\"present_lon\", \"present_lat\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5da1c1a7",
   "metadata": {
    "papermill": {
     "duration": 0.002934,
     "end_time": "2024-02-01T04:38:40.818846",
     "exception": false,
     "start_time": "2024-02-01T04:38:40.815912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Assign subduction data to point deposit/non-deposit/unlabelled data\n",
    "\n",
    "Here we assign the appropriate values for the subduction-related parameters (kinematics, seafloor age, etc.) to the deposit sites and random locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f932fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:38:40.827705Z",
     "iopub.status.busy": "2024-02-01T04:38:40.827441Z",
     "iopub.status.idle": "2024-02-01T04:38:49.874098Z",
     "shell.execute_reply": "2024-02-01T04:38:49.873550Z"
    },
    "papermill": {
     "duration": 9.052644,
     "end_time": "2024-02-01T04:38:49.875831",
     "exception": false,
     "start_time": "2024-02-01T04:38:40.823187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "coregistered_data = run_coregister_combined_point_data(\n",
    "    point_data=combined_points,\n",
    "    subduction_data=subduction_data,\n",
    "    n_jobs=n_jobs,\n",
    "    verbose=verbose,\n",
    ")\n",
    "del combined_points, subduction_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "049e3a37",
   "metadata": {
    "papermill": {
     "duration": 0.003226,
     "end_time": "2024-02-01T04:38:49.882554",
     "exception": false,
     "start_time": "2024-02-01T04:38:49.879328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Assign crustal thickness data to point data\n",
    "\n",
    "This cell extracts the overriding plate thickness at each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63ca008c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:38:49.889717Z",
     "iopub.status.busy": "2024-02-01T04:38:49.889467Z",
     "iopub.status.idle": "2024-02-01T04:39:52.417872Z",
     "shell.execute_reply": "2024-02-01T04:39:52.417186Z"
    },
    "papermill": {
     "duration": 62.534356,
     "end_time": "2024-02-01T04:39:52.419860",
     "exception": false,
     "start_time": "2024-02-01T04:38:49.885504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "coregistered_data = run_coregister_crustal_thickness(\n",
    "    point_data=coregistered_data,\n",
    "    input_dir=crustal_thickness_dir,\n",
    "    n_jobs=n_jobs,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930468be",
   "metadata": {
    "papermill": {
     "duration": 0.00347,
     "end_time": "2024-02-01T04:39:52.428717",
     "exception": false,
     "start_time": "2024-02-01T04:39:52.425247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Calculate cumulative erosion\n",
    "\n",
    "Here we calculate the cumulative erosion experienced by each deposit/random point since its time of formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15fe4a5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:39:52.437782Z",
     "iopub.status.busy": "2024-02-01T04:39:52.437533Z",
     "iopub.status.idle": "2024-02-01T04:40:34.026901Z",
     "shell.execute_reply": "2024-02-01T04:40:34.026085Z"
    },
    "papermill": {
     "duration": 41.596609,
     "end_time": "2024-02-01T04:40:34.028888",
     "exception": false,
     "start_time": "2024-02-01T04:39:52.432279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "coregistered_data = calculate_erodep(\n",
    "    coregistered_data,\n",
    "    input_dir=erodep_dir,\n",
    "    n_jobs=n_jobs,\n",
    "    column_name=\"erosion (m)\",\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0d00f",
   "metadata": {
    "papermill": {
     "duration": 0.005218,
     "end_time": "2024-02-01T04:40:34.070690",
     "exception": false,
     "start_time": "2024-02-01T04:40:34.065472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Assign data to regions\n",
    "\n",
    "To divide the data into individual regions for the later analysis, we use the `regions_filename` defined earlier, if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98a93d0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:40:34.080226Z",
     "iopub.status.busy": "2024-02-01T04:40:34.079924Z",
     "iopub.status.idle": "2024-02-01T04:40:34.262097Z",
     "shell.execute_reply": "2024-02-01T04:40:34.261458Z"
    },
    "papermill": {
     "duration": 0.189401,
     "end_time": "2024-02-01T04:40:34.263909",
     "exception": false,
     "start_time": "2024-02-01T04:40:34.074508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if regions_filename is not None and os.path.isfile(regions_filename):\n",
    "    points = gpd.GeoSeries.from_xy(\n",
    "        coregistered_data[\"present_lon\"],\n",
    "        coregistered_data[\"present_lat\"],\n",
    "        index=coregistered_data.index,\n",
    "    )\n",
    "    coregistered_data[\"region\"] = assign_regions(\n",
    "        points,\n",
    "        regions=regions_filename,\n",
    "    )\n",
    "    del points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be29d4",
   "metadata": {},
   "source": [
    "### Save to file\n",
    "\n",
    "Finally, we write the dataset to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3de5c45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:40:34.271230Z",
     "iopub.status.busy": "2024-02-01T04:40:34.270990Z",
     "iopub.status.idle": "2024-02-01T04:40:36.456667Z",
     "shell.execute_reply": "2024-02-01T04:40:36.456072Z"
    },
    "papermill": {
     "duration": 2.191004,
     "end_time": "2024-02-01T04:40:36.458240",
     "exception": false,
     "start_time": "2024-02-01T04:40:34.267236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region          label     \n",
       "East Asia       negative         6\n",
       "                positive         7\n",
       "                unlabelled    5089\n",
       "North America   negative        56\n",
       "                positive       308\n",
       "                unlabelled    5604\n",
       "Other           negative       169\n",
       "                positive         1\n",
       "                unlabelled    4022\n",
       "South America   negative       702\n",
       "                positive       236\n",
       "                unlabelled    5077\n",
       "Southeast Asia  negative         4\n",
       "                positive        57\n",
       "                unlabelled    7487\n",
       "Tethys          positive        75\n",
       "                unlabelled    6939\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coregistered_data.to_csv(output_filename, index=False)\n",
    "\n",
    "coregistered_data.groupby([\"region\", \"label\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78df4c5",
   "metadata": {
    "papermill": {
     "duration": 0.005064,
     "end_time": "2024-02-01T04:40:36.469419",
     "exception": false,
     "start_time": "2024-02-01T04:40:36.464355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1237.794352,
   "end_time": "2024-02-01T04:40:39.294002",
   "environment_variables": {},
   "exception": null,
   "input_path": "00c-extract_training_data_global.ipynb",
   "output_path": "00c-extract_training_data_global.ipynb",
   "parameters": {},
   "start_time": "2024-02-01T04:20:01.499650",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
